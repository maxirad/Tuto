{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Scikit-Learn\n",
    "\n",
    "## Dimension Reduction\n",
    "\n",
    "### PCA\n",
    "\n",
    "https://scikit-learn.org/stable/modules/decomposition.html\n",
    "The point is to find the successive orthogonal components that explain most of the variance of the centered data set.\n",
    "Here is a very simple video on the Topic https://www.youtube.com/watch?v=FgakZw6K1QQ\n",
    "\n",
    "Here is the scikit-learn documentation\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=[# , 'mle', %])\n",
    "    pca.fit(X)\n",
    "\n",
    "you can specify in n_components\n",
    "* number of features to keep\n",
    "* 'mle' to let Minka's MLE algorithm fit it for you https://vismod.media.mit.edu/tech-reports/TR-514.pdf\n",
    "* a percentage between 0 and 1 that represents the amount of total variance that should be explained by your features\n",
    "\n",
    "Useful attributes\n",
    "* components_ : array, shape (n_components, n_features) -- Gives you the n_components components (rows) and the contribution of each feature (columns)\n",
    "* explained_variance_ (ratio_) : array, shape (n_components,) -- Gives you the variance explained by each component\n",
    "\n",
    "Some Methods\n",
    "* fit(X) : fits the model with X\n",
    "* fit_transform(X) : fits AND returns the transformed data\n",
    "* transform(X) : returns the transformed data using the fitted model\n",
    "* inverse_transform(X) : transform your data back to the original space\n",
    "* get_covariance() : computes the covariance matrix $cov \\in \\mathscr{M}_{n_{features}}$  \n",
    "$$cov =  components^T * S^2 * components + \\boldsymbol{\\sigma_2} * I_{n_{features}}$$ \n",
    "where $S^2$ contains the explained variances, and $\\boldsymbol{\\sigma_2}$ contains the noise variances.\n",
    "* get_precision() : computes the precision (inverse of the covariance)\n",
    "\n",
    "If you're inteerested in only a certain part of the whole dataset you can use the \n",
    "* svd_solver='randomized' : it only uses the right amount of data to predict the n_features wanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_components' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-639c946718d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## X is the dataset : lines are instances, columns are features ##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mX_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_components' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "## X is the dataset : lines are instances, columns are features ##\n",
    "\n",
    "pca = PCA(n_components).fit(X)\n",
    "X_pca = PCA(n_components).transform(X)\n",
    "\n",
    "X_pca = PCA(n_components).fit_transform(X)\n",
    "\n",
    "# This function plots an elbow curve representing the variance explained by components\n",
    "def plot_elbow(X,n_components=10):\n",
    "    pca = PCA(n_components).fit(X)\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('number of components')\n",
    "    plt.ylabel('cumulative explained variance')\n",
    "    plt.title('Ratio of variance explained by the number components')\n",
    "    plt.show()\n",
    "    \n",
    "#A more general implementation for visualizing data is available under Kernel PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incremental PCA\n",
    "\n",
    "For big sized data you would want to use chunks of data.\n",
    "It computes estimates of components and naoise variances from a batch and then updates them with the next batch <br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel PCA\n",
    "\n",
    "Documentation : https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html\n",
    "\n",
    "- You can use a special kernel to separate non linear datasets : https://scikit-learn.org/stable/modules/metrics.html\n",
    "\n",
    "    - Linear : $$ K(x,x') = x^Tx' $$\n",
    "    - poly : $$ K(x,x') = ( \\color {green} \\gamma x^T x' + \\color {blue} c_0)^\\color {red}d $$\n",
    "    - sigmoid : $$ K(x,x') = tanh( \\color {green} \\gamma x^T x' + \\color {blue} c_0 ) \\;\\;\\; $$\n",
    "    - Radial basis function (RBF) : $$ K(x,x') = exp(- \\color {green} \\gamma \\|{x-x'}\\|^2) $$\n",
    "    - cosine : $$ K(x,x') = \\frac {x^T x'}{\\|x^T\\| \\|x'\\|} $$\n",
    "\n",
    "You can tune some Hyper parameter\n",
    "\n",
    "$\\color {green} \\gamma $ <br>\n",
    "`gamma  (default = 1/n_features) is used by poly / sigmoid / rbf`<br>\n",
    "$\\color {blue} {c_0} $ <br>\n",
    "`coef0  (default = 1)            is used by poly / sigmoid` <br>\n",
    "$\\color {red} d $ <br>\n",
    "`degree (default = 3)            is used by poly`<br>\n",
    "\n",
    "\n",
    "More info on kernels : http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# This function plots the projection of the data on the 1 2 or 3 main components and returns the PCA\n",
    "#using whichever kernel and parameter you give it\n",
    "\n",
    "def plot_pca (X,y,kernel='linear',n_components=2,gamma=None,coef0=None,degree=None):\n",
    "    pca = KernelPCA(n_components,kernel, gamma=gamma, degree=degree, coef0=coef0)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    print(\"original shape:   \", X.shape)\n",
    "    print(\"transformed shape:\", X_pca.shape)\n",
    "    if n_components==1:\n",
    "        plt.scatter(X_pca[:,0],np.zeros(len(X_pca),),alpha=0.2,c=y.values,vmin=-3,vmax=3,)\n",
    "        plt.xlabel('Component 1')\n",
    "        plt.title(\"data projected on the main component \\n using \" + kernel + \" kernel\")\n",
    "    elif n_components==2:\n",
    "        plt.scatter(X_pca[:,0],X_pca[:,1],alpha=0.2,c=y.values,vmin=-3,vmax=3)\n",
    "        plt.xlabel('Component 1')\n",
    "        plt.ylabel('Component 2')\n",
    "        plt.title(\"data projected on the 2 main components \\n using \" + kernel + \" kernel\")\n",
    "    elif n_components==3:\n",
    "        from mpl_toolkits.mplot3d import Axes3D\n",
    "        fig=plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(X_pca[:,0],X_pca[:,1],X_pca[:,2],alpha=0.2,c=y.values,vmin=-3,vmax=3)\n",
    "        ax.set_xlabel('Component 1')\n",
    "        ax.set_ylabel('Component 2')\n",
    "        ax.set_zlabel('Component 3')\n",
    "        plt.title(\"data projected on the 3 main components \\n using \" + kernel + \" kernel\")\n",
    "        return pca\n",
    "    else :\n",
    "        print(\"how am I supposed to show you that with your 2-D eyes, beta !\")\n",
    "        return pca\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.show()\n",
    "    return pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse PCA\n",
    "\n",
    "You can use Sparse PCA to yield sparse component, this is used via a Lasso ($l_1$) regularization\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparsePCA.html#sklearn.decomposition.SparsePCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truncated SVD\n",
    "\n",
    "If you have a large sparse dataset that you don't want to center (because of Out Of Memory Error) use this algorithm (ex : tf-idf count matrices)\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isomap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-Distributed Stochastic Neighbor Embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 2) (6,)\n",
      "TRAINindex: [2 3 4 5] TESTindex: [0 1]\n",
      "TrainSet: \n",
      " [[ 5  6]\n",
      " [ 7  8]\n",
      " [ 9 10]\n",
      " [11 12]] \n",
      " [2 3 4 5] \n",
      " TestSet: \n",
      " [[1 2]\n",
      " [3 4]] \n",
      " [0 1]\n",
      "TRAINindex: [0 1 4 5] TESTindex: [2 3]\n",
      "TrainSet: \n",
      " [[ 1  2]\n",
      " [ 3  4]\n",
      " [ 9 10]\n",
      " [11 12]] \n",
      " [0 1 4 5] \n",
      " TestSet: \n",
      " [[5 6]\n",
      " [7 8]] \n",
      " [2 3]\n",
      "TRAINindex: [0 1 2 3] TESTindex: [4 5]\n",
      "TrainSet: \n",
      " [[1 2]\n",
      " [3 4]\n",
      " [5 6]\n",
      " [7 8]] \n",
      " [0 1 2 3] \n",
      " TestSet: \n",
      " [[ 9 10]\n",
      " [11 12]] \n",
      " [4 5]\n",
      "Estimated Accuracy nan (nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrator/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/administrator/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/administrator/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/home/administrator/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/administrator/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9,10], [11,12]])  #your EARLY dataset\n",
    "y = np.array([0, 1, 2, 3, 4, 5])                              #your PREDICTED dataset\n",
    "kf = KFold(n_splits=3)   #do a 3 fold\n",
    "print(X.shape, y.shape)\n",
    "scores=list()\n",
    "\n",
    "for train_index, test_index in kf.split(X,y):\n",
    "    print(\"TRAINindex:\", train_index, \"TESTindex:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(\"TrainSet: \\n\", X_train, \"\\n\", y_train,\"\\n TestSet: \\n\",X_test, \"\\n\",y_test)\n",
    "    \n",
    "    # DEFINE A MODEL HERE\n",
    "    \n",
    "    # FIT A MODEL HERE ON X_TRAIN + y_train\n",
    "    \n",
    "    # EVALUATE MODEL HERE X_TEST + y_test\n",
    "    \n",
    "    # STORE THE RESULTS in a list scores=list() scores.append(accuracy,loss)\n",
    "    \n",
    "print('Estimated Accuracy %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
