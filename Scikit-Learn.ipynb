{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Scikit-Learn\n",
    "\n",
    "# Unsupervised Learning\n",
    "\n",
    "## [Dimension Reduction](https://scikit-learn.org/stable/modules/decomposition.html)\n",
    "\n",
    "### Principal Component Analysis ( [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) )\n",
    "\n",
    "The point is to find the successive orthogonal components that explain most of the variance of the centered data set.\n",
    "Here is a very simple video on the Topic https://www.youtube.com/watch?v=FgakZw6K1QQ\n",
    "\n",
    "you can specify in n_components\n",
    "* number of features to keep\n",
    "* 'mle' to let Minka's MLE algorithm fit it for you https://vismod.media.mit.edu/tech-reports/TR-514.pdf\n",
    "* a percentage between 0 and 1 that represents the amount of total variance that should be explained by your features\n",
    "\n",
    "Useful attributes\n",
    "* components_ : array, shape ($n_{components}\\;, n_{features}$) -- Gives you the n_components components (rows) and the contribution of each feature (columns)\n",
    "* explained_variance_ (ratio_) : array, shape ($n_{components}$,) -- Gives you the variance explained by each component\n",
    "\n",
    "Some Methods\n",
    "* fit(X) : fits the model with X\n",
    "* fit_transform(X) : fits AND returns the transformed data\n",
    "* transform(X) : returns the transformed data using the fitted model\n",
    "* inverse_transform(X) : transform your data back to the original space\n",
    "* get_covariance() : computes the covariance matrix $cov \\in \\mathscr{M}_{n_{features}}$  \n",
    "$$cov =  components^T * S^2 * components + \\boldsymbol{\\sigma_2} * I_{n_{features}}$$ \n",
    "where $S^2$ contains the explained variances, and $\\boldsymbol{\\sigma_2}$ contains the noise variances.\n",
    "* get_precision() : computes the precision (inverse of the covariance)\n",
    "\n",
    "If you're inteerested in only a certain part of the whole dataset you can use the \n",
    "* svd_solver='randomized' : it only uses the right amount of data to predict the n_features wanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "## X is the dataset : lines are instances, columns are features ##\n",
    "\n",
    "pca = PCA(n_components).fit(X)\n",
    "X_pca = PCA(n_components).transform(X)\n",
    "\n",
    "X_pca = PCA(n_components).fit_transform(X)\n",
    "\n",
    "# This function plots an elbow curve representing the variance explained by components\n",
    "def plot_elbow(X,n_components=10):\n",
    "    pca = PCA(n_components).fit(X)\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('number of components')\n",
    "    plt.ylabel('cumulative explained variance')\n",
    "    plt.title('Ratio of variance explained by the number components')\n",
    "    plt.show()\n",
    "    \n",
    "#A more general implementation for visualizing data is available under Kernel PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Incremental PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html)\n",
    "\n",
    "For big sized data you would want to use chunks of data.\n",
    "It computes estimates of components and naoise variances from a batch and then updates them with the next batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Kernel PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html)\n",
    "\n",
    "- You can use a special kernel to separate non linear datasets : https://scikit-learn.org/stable/modules/metrics.html\n",
    "\n",
    "    - Linear : $$ K(x,x') = x^Tx' $$\n",
    "    - poly : $$ K(x,x') = ( \\color {green} \\gamma x^T x' + \\color {blue} c_0)^\\color {red}d $$\n",
    "    - sigmoid : $$ K(x,x') = tanh( \\color {green} \\gamma x^T x' + \\color {blue} c_0 ) \\;\\;\\; $$\n",
    "    - Radial basis function (RBF) : $$ K(x,x') = exp(- \\color {green} \\gamma \\|{x-x'}\\|^2) $$\n",
    "    - cosine : $$ K(x,x') = \\frac {x^T x'}{\\|x^T\\| \\|x'\\|} $$\n",
    "\n",
    "You can tune some Hyper parameter\n",
    "\n",
    "$\\color {green} \\gamma $ <br>\n",
    "`gamma  (default = 1/n_features) is used by poly / sigmoid / rbf`<br>\n",
    "$\\color {blue} {c_0} $ <br>\n",
    "`coef0  (default = 1)            is used by poly / sigmoid` <br>\n",
    "$\\color {red} d $ <br>\n",
    "`degree (default = 3)            is used by poly`<br>\n",
    "\n",
    "\n",
    "More info on kernels : http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# This function plots the projection of the data on the 1 2 or 3 main components and returns the PCA\n",
    "#using whichever kernel and parameter you give it\n",
    "\n",
    "def plot_pca (X,y,kernel='linear',n_components=2,gamma=None,coef0=None,degree=None):\n",
    "    pca = KernelPCA(n_components,kernel, gamma=gamma, degree=degree, coef0=coef0)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    print(\"original shape:   \", X.shape)\n",
    "    print(\"transformed shape:\", X_pca.shape)\n",
    "    if n_components==1:\n",
    "        plt.scatter(X_pca[:,0],np.zeros(len(X_pca),),alpha=0.2,c=y.values,vmin=-3,vmax=3,)\n",
    "        plt.xlabel('Component 1')\n",
    "        plt.title(\"data projected on the main component \\n using \" + kernel + \" kernel\")\n",
    "    elif n_components==2:\n",
    "        plt.scatter(X_pca[:,0],X_pca[:,1],alpha=0.2,c=y.values,vmin=-3,vmax=3)\n",
    "        plt.xlabel('Component 1')\n",
    "        plt.ylabel('Component 2')\n",
    "        plt.title(\"data projected on the 2 main components \\n using \" + kernel + \" kernel\")\n",
    "    elif n_components==3:\n",
    "        from mpl_toolkits.mplot3d import Axes3D\n",
    "        fig=plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(X_pca[:,0],X_pca[:,1],X_pca[:,2],alpha=0.2,c=y.values,vmin=-3,vmax=3)\n",
    "        ax.set_xlabel('Component 1')\n",
    "        ax.set_ylabel('Component 2')\n",
    "        ax.set_zlabel('Component 3')\n",
    "        plt.title(\"data projected on the 3 main components \\n using \" + kernel + \" kernel\")\n",
    "        return pca\n",
    "    else :\n",
    "        print(\"how am I supposed to show you that with your 2-D eyes, beta !\")\n",
    "        return pca\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.show()\n",
    "    return pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Sparse PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparsePCA.html#sklearn.decomposition.SparsePCA)\n",
    "\n",
    "You can use Sparse PCA to yield sparse component, this is used via a Lasso ($l_1$) regularization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Truncated SVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)\n",
    "\n",
    "If you have a large sparse dataset that you don't want to center (because of Out Of Memory Error) use this algorithm (ex : tf-idf count matrices)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally Linear Embedding ( LLE )\n",
    "\n",
    "Manifold Learning technique : Learning locally linear space for k closest neighbor $LLE : x^{(i)} \\to z^{(i)}$\n",
    "- Selects k closest neighbors \n",
    "$$ K_\\boldsymbol{x^{(i)}} = \\underset{(\\boldsymbol x^{(j)})_{j\\in[[1:k]]}}{\\operatorname{argmin}}\n",
    "(\\sum\\limits_{j=1}^k d(x^{(i)}-x^{(j)})) $$\n",
    "- Optimizes the weights for the locally linear relations (constructing linear model for each k subset)\n",
    "$$ \\boldsymbol {\\hat W} = \\underset{\\boldsymbol W \\in \\mathscr M_m}{\\operatorname{argmin}}\n",
    "\\sum\\limits_{i=1}^m \\| \\boldsymbol {x^{(i)}} - \\sum\\limits_{j=1}^m w_{i,j} \\boldsymbol{x^{(j)}} \\|^2  $$ \n",
    "$$ \\text{ where } w_{i,j}=0 \\text{ if } \\boldsymbol{x^{(j)}} \\not\\in K_\\boldsymbol{x^{(i)}} \n",
    "\\text{ and } \\sum\\limits_{j=1}^m w_{i,j}=1 $$\n",
    "- Minimizes the distance between the closest neighbourg (constructing low dimensional representation)\n",
    "$$ \\boldsymbol {\\hat Z} = \\underset{\\boldsymbol Z \\in \\mathscr M_m}{\\operatorname{argmin}}\n",
    "\\sum\\limits_{i=1}^m \\| \\boldsymbol {z^{(i)}} - \\sum\\limits_{j=1}^m \\hat{w}_{i,j} \\boldsymbol{z^{(j)}} \\|^2  $$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiDimensional Scaling ( MDS )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isomap\n",
    "\n",
    "Creates a graph and reduces dimensionality by preserving geodesic distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-Distributed Stochastic Neighbor Embedding ( t-SNE )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis ( LDA )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent Component Analysis ( ICA )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Negative Matrix Factorization ( NMF )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Clustering](https://scikit-learn.org/stable/modules/clustering.html)\n",
    "\n",
    "### [K-means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "\n",
    "![K-means algorithm example](https://cdn-images-1.medium.com/max/800/1*KrcZK0xYgTa4qFrVr0fO2w.gif)\n",
    "\n",
    "K-means algorithm and [a nice visualization](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/) :\n",
    "   1. Arbitrarly defining K centroids points\n",
    "   2. Labelling each point according to their closest centroid\n",
    "   3. Putting the new centroid at the mean of all the points of each cluster (same label) points\n",
    "   4. Repeating 2 and 3 until the centroids converge <br>\n",
    "Complexity = $O(n)$\n",
    "\n",
    "Useful Parameters :\n",
    "- n_clusters (int) : K of the K means\n",
    "- init : \"[k-means++](https://en.wikipedia.org/wiki/K-means%2B%2B)\" (default), \"random\" if you want to initialize your centroids using random points, or an array of initializing points\n",
    "- n_init (int) : number of times the k means will be run with new initial centroids\n",
    "- max_iter (int) : number of repetition of 2 and 3\n",
    "- tol (float) : limit to declare the convergence of the centroid $(if \\ inertia_{t+1}-inertia_{t}<tol)$\n",
    "- n_jobs (int) : lets you define how many CPU you want to use for computation, -1 means all, None (default) is equal to 1 by default\n",
    "\n",
    "Attributes :\n",
    "- cluster_centers_ (array) : gives the position of all your clusters, shape : [$n_{clusters}$ \\, $n_{features}$]\n",
    "- labels_ (list) : Labels of each point given to fit()\n",
    "- inertia_ (float) : Sum of squared distances of samples to their closest cluster center\n",
    "- n_iter_ (int) : Number of iterations run.\n",
    "\n",
    "Methods :\n",
    "- fit(X) : Runs the algorithm to find the clusters\n",
    "- transform(X) : Transforms your data to a distance from its centroid space\n",
    "- predict(X) : Predicts to which cluster X will belong to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=n_clusters,n_jobs=-1).fit(x)\n",
    "labels = kmeans.labels_\n",
    "silhouette = metrics.silhouette_score(x, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Mean Shift](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html)\n",
    "\n",
    "![Mean shift search](https://cdn-images-1.medium.com/max/800/1*bkFlVrrm4HACGfUzeBnErw.gif)\n",
    "![Mean shift algorithm](https://cdn-images-1.medium.com/max/800/1*vyz94J_76dsVToaa4VG1Zg.gif)\n",
    "\n",
    "Mean shift algorithm :\n",
    "   1. Arbitrarly defining n centroids\n",
    "   2. Find all points lying in the neighbourhood of each centroids defined by the bandwith\n",
    "   3. Putting the new centroids at the mean of all the points of a neighbourhood\n",
    "   4. Repeating 2. and 3. until centroids converge<br>\n",
    "Complexity = $O(n log(n))$\n",
    "\n",
    "Parameters :\n",
    "- bandwidth (float) : bandwidth that defines the neighbourhood, if not given estimated using sklearn.cluster.estimate_bandwidth (does not scale well)\n",
    "- seeds (array) : seeds to use for initial centroids, if not given clustering.get_bin_seeds makes a seed grid using bandwidth as size.\n",
    "- bin_seeding (boolean) : False (default) means all the seeds points are used, silenced if seeds are not given\n",
    "- min_bin_freq (int) : Minimal number of points in the bin to induce a seed, default is 1\n",
    "- cluster_all (boolean) : True (default) forces the clusterization of all points\n",
    "- n_jobs (int) : lets you define how many CPU you want to use for computation, -1 means all, None (default) is equal to 1 by default\n",
    "\n",
    "Attributes :\n",
    "- cluster_centers_ (array) : gives the position of all your clusters, shape : [$n_{clusters}$ \\, $n_{features}$]\n",
    "- labels_ (list) : Labels of each point given to fit()\n",
    "\n",
    "Methods :\n",
    "- fit(X) : Runs the algorithm to find the clusters\n",
    "- predict(X) : Predicts to which cluster X will belong to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Piping K-Means and Mean Shift](http://jamesxli.blogspot.com/2012/03/on-mean-shift-and-k-means-clustering.html)\n",
    "\n",
    "By running a K-Means with a K bigger than the real number of clusters. You can produce the seeds for a Mean Shift.\n",
    "\n",
    "![Piping K-means and Mean Shift](http://1.bp.blogspot.com/-4-yQSzGmCC8/T1aNsa6nJJI/AAAAAAAAH00/SWyAy7MC69M/s640/MapClustering3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density Based Spatial Clustering of Applications with Noise ( [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) ) \n",
    "\n",
    "![DBSCAN illustration](https://cdn-images-1.medium.com/max/800/1*tc8UF-h0nQqUfLC8-0uInQ.gif)\n",
    "\n",
    "DBSCAN algorithm : Here is [a nice visualization](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)\n",
    "   1. for a data point, get all the points that are within $ \\epsilon $ reach\n",
    "   2. if there are more then min_samples :\n",
    "       - a cluster with all the points is formed and can be broaden with the other points\n",
    "       - otherwise the point is tagged as noise\n",
    "   3. repeat 1. and 2. for all the points<br>\n",
    "Complexity = $O(n)$\n",
    "   \n",
    "Useful parameters :\n",
    "- eps (float) : reach $\\epsilon$ of the neighbourhood of a point\n",
    "- min_samples (int) : Minimum number of points to declare a cluster\n",
    "- metric (string) : option from [sklearn.metrics.pairwise_distances](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html), [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’, ‘manhattan’]\n",
    "- n_jobs (int) : lets you define how many CPU you want to use for computation, -1 means all, None (default) is equal to 1 by default\n",
    "\n",
    "Attributes :\n",
    "- core_sample_indices_ (array) : Indices of core samples.\n",
    "- components_ (array) : Coordinates of each core sample found by training, shape = $[n_{core samples}\\ , n_{features}]$\n",
    "- labels_ (array) : Cluster labels for each point in the dataset given to fit(). Noisy samples are given the label -1.\n",
    "\n",
    "Methods :\n",
    "- fit(X) : Runs the algorithm to find the clusters\n",
    "- fit_predict(X) : Runs the algorithm to find the clusters and returns the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=eps, min_samples=min_s).fit(X)\n",
    "labels=db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordering Points To Identify the Clustering Structure ( [OPTICS](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.OPTICS.html) ) \n",
    "\n",
    "OPTICS algorithm : [Here is a nice illustration](https://www.youtube.com/watch?v=8kJjgowewOs)\n",
    "   1. Computing _Core Distances_ for each point of the dataset ($\\epsilon$ of the neighbourhood with min_samples)\n",
    "   2. Choosing a point, set i to 0\n",
    "   3. Setting its $order=i$\n",
    "   4. Setting the $reachability = max(Core \\ Distance \\ , \\   distance_{closest \\ neighbour})$\n",
    "   5. Incrementing $i=i+1$ \n",
    "   6. Repeating 3. 4. and 5. using the closest neighbour<br>\n",
    "Complexity = $O(n^2)$\n",
    "\n",
    "You then get a reachability plot :\n",
    "![Reachability plot](https://scikit-learn.org/stable/_images/sphx_glr_plot_optics_001.png)\n",
    "\n",
    "\n",
    "A valley represents a cluster. A valley into another valley represents a nested cluster. <br>\n",
    "You can represent the behaviour of DBSCAN by putting a threshold at a certain level and seperating the cluster everytime the reachability of the next point (which is the closest remember) is exceeding the threshold\n",
    "\n",
    "Useful parameters :\n",
    "- max_eps (float) : maximum reachability to look for\n",
    "- min_samples (int) : Minimum number of points to declare a cluster\n",
    "- metric (string) : option from [sklearn.metrics.pairwise_distances](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html), [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’, ‘manhattan’]\n",
    "- cluster_method (string) : ['xi', 'dbscan'] method to create the cluster from the reachability plot\n",
    "- eps (float) : if cluster_method='dbscan', threshold to declare a point as noise\n",
    "- xi (float) : if cluster_method='xi', determines steepness to declare further points as noise $\\frac{x_i}{x_{i+1}}<1-xi$\n",
    "- n_jobs (int) : lets you define how many CPU you want to use for computation, -1 means all, None (default) is equal to 1 by default\n",
    "\n",
    "USeful Attributes :\n",
    "- labels_ (array) : Cluster labels for each point in the dataset given to fit(). Noisy samples are given the label -1.\n",
    "- reachability_ (array) : Reachability distances per sample, indexed by object order. \n",
    "- ordering_ (array) : The cluster ordered list of sample indices. Use clust.reachability_[clust.ordering_] to access in cluster order.\n",
    "\n",
    "Methods :\n",
    "- fit(X) : Runs the algorithm to find the clusters\n",
    "- fit_predict(X) : Runs the algorithm to find the clusters and returns the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples=100\n",
    "title=\"reachability plot given a minimum samples of \"+ str(min_samples)\n",
    "\n",
    "clust = OPTICS(min_samples=min_samples)\n",
    "clust.fit(x)\n",
    "\n",
    "reachability = clust.reachability_[clust.ordering_]\n",
    "\n",
    "plt.plot(reachability)\n",
    "plt.title(title)\n",
    "plt.xlabel(\"order\")\n",
    "plt.ylabel(\"reachability\")\n",
    "plt.ylim(top=reachability[1:].mean()+3*reachability[1:].std())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Expectation Maximization ( EM ) using Gaussian Mixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)\n",
    "\n",
    "![EM Clustering viz](https://cdn-images-1.medium.com/max/800/1*OyXgise21a23D5JCss8Tlg.gif)\n",
    "\n",
    "EM algorithm :\n",
    "   1. Set K Gaussian with random means and covariance matrices\n",
    "   2. Compute for every point the probability that it belongs to the gaussian\n",
    "   3. Put the mean to the weighted (by the probability of each points) mean of all points and adapt the covariance to maximize the likelihood of the Gaussian given the data\n",
    "   4. Repeat 2. and 3. until convergence<br>\n",
    "   Complexity = $O(n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Agglomerative Clustering ( [HAC](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) )\n",
    "\n",
    "This clustering is a bottom up agglomerative algorithm (starts from a unique point and ends on a big cluster containing every points)\n",
    "\n",
    "![Hierarchical Clustering viz](https://cdn-images-1.medium.com/max/800/1*ET8kCcPpr893vNZFs8j4xg.gif)\n",
    "\n",
    "HAC algorithm :\n",
    "   1. Each point is treated as a cluster and every distance to each cluster is computed\n",
    "   2. The two clusters with the smaller distance are then merged into a unique cluster\n",
    "   3. Repeat until there is only one cluster left<br>\n",
    "   Complexity = $O(n^3)$\n",
    "\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Model Selection](https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html)\n",
    "\n",
    "## [Grid Search](https://scikit-learn.org/stable/modules/grid_search.html)\n",
    "\n",
    "A Grid Search is used to fine the best hyperparameter for your model.\n",
    "\n",
    "* a parameter space (which parameters of your are you gonna tune)\n",
    "* a method for searching and sampling candidates (which values are the parameters gonna take)\n",
    "* an estimator (what regressor or classifier will make the predictions)\n",
    "* a score function (how are you gonna measure which model is better)\n",
    "* a cross validation scheme (for unbiased estimator you have to cross validate)\n",
    "\n",
    "### Defining a grid of parameters\n",
    "\n",
    "Here is a standard parameter grid for a kernel PCA decomposition problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'pca__kernel': ['linear','cosine'],\n",
    "     'pca__n_components':[1,2,3,4,5,6]},\n",
    "    {'pca__kernel': ['rbf'], \n",
    "     'pca__gamma':[10**-6, 10**-5, 10**-4, 0.001, 0.01, 0.1, 1, 10],\n",
    "     'pca__n_components':[1,2,3,4,5,6]},\n",
    "    {'pca__kernel': ['sigmoid'],\n",
    "     'pca__gamma':[-10**-6,-10**-5,-10**-4,-0.001,-0.01,-0.1,-1,-10,\n",
    "                    10**-6, 10**-5, 10**-4, 0.001, 0.01, 0.1, 1, 10],\n",
    "     'pca__coef0':[-100,-10,-5,-1,-0.1,0,\n",
    "                    100, 10, 5, 1, 0.1],\n",
    "     'pca__n_components':[1,2,3,4,5,6]},\n",
    "    {'pca__kernel': ['poly'],\n",
    "     'pca__gamma':[10**-6, 10**-5, 10**-4, 0.001, 0.01, 0.1, 1, 10],\n",
    "     'pca__coef0':[-100,-10,-5,-1,-0.1,0,\n",
    "                    100, 10, 5, 1, 0.1],\n",
    "     'pca__degree':[-5,-4,-3,-2,-1,-0.5,0.5,2,3,4,5],\n",
    "     'pca__n_components':[1,2,3,4,5,6]},\n",
    " ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the Grid Search\n",
    "\n",
    "[GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "lets you choose several of the options you want for your Grid Search\n",
    "\n",
    "* estimator (object with a score function)\n",
    "* param_grid (dict)\n",
    "* n_jobs (int) : number of jobs to run in parallel : -1 sets maximum\n",
    "* cv (int) : number of fold for the Kfold or Stratified Kfold (default) or cv method\n",
    "* verbose (int) : 0 (no output) 1(some outout) 2(every CV time output) 3(CV time + score output)\n",
    "\n",
    "Useful attribute :\n",
    "* cv_results : Dict with results\n",
    "* best_estimator_ : object estimator with the parameters that yielded the best score\n",
    "\n",
    "Methods :\n",
    "* fit(X,y) : Runs fits for all the parameters\n",
    "* transform(X) : Runs transform of X for the best estimator\n",
    "* predict(X) : Runs predict of X using the best estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Cross validation](https://scikit-learn.org/stable/modules/cross_validation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9,10], [11,12]])  #your EARLY dataset\n",
    "y = np.array([0, 1, 2, 3, 4, 5])                              #your PREDICTED dataset\n",
    "kf = KFold(n_splits=3)   #do a 3 fold\n",
    "print(X.shape, y.shape)\n",
    "scores=list()\n",
    "\n",
    "for train_index, test_index in kf.split(X,y):\n",
    "    print(\"TRAINindex:\", train_index, \"TESTindex:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(\"TrainSet: \\n\", X_train, \"\\n\", y_train,\"\\n TestSet: \\n\",X_test, \"\\n\",y_test)\n",
    "    \n",
    "    # DEFINE A MODEL HERE\n",
    "    \n",
    "    # FIT A MODEL HERE ON X_TRAIN + y_train\n",
    "    \n",
    "    # EVALUATE MODEL HERE X_TEST + y_test\n",
    "    \n",
    "    # STORE THE RESULTS in a list scores=list() scores.append(accuracy,loss)\n",
    "    \n",
    "print('Estimated Accuracy %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
