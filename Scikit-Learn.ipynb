{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Scikit-Learn\n",
    "\n",
    "## Supervised Learning\n",
    "\n",
    "### PCA\n",
    "\n",
    "https://scikit-learn.org/stable/modules/decomposition.html#pca\n",
    "The point is to find the successive orthogonal components that explain most of the variance of the centered data set.\n",
    "Here is a very simple video on the Topic https://www.youtube.com/watch?v=FgakZw6K1QQ\n",
    "\n",
    "Here is the scikit-learn documentation\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA\n",
    "\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=[# , 'mle', %])\n",
    "    pca.fit(X)\n",
    "\n",
    "you can specify in n_components\n",
    "* number of features to keep\n",
    "* 'mle' to let Minka's MLE algorithm fit it for you https://vismod.media.mit.edu/tech-reports/TR-514.pdf\n",
    "* a percentage between 0 and 1 that represents the amount of total variance that should be explained by your features\n",
    "\n",
    "Useful attributes\n",
    "* components_ : array, shape (n_components, n_features) -- Gives you the n_components components (rows) and the contribution of each feature (columns)\n",
    "* explained_variance_ (ratio_) : array, shape (n_components,) -- Gives you the variance explained by each component\n",
    "\n",
    "Some Methods\n",
    "* fit(X) : fits the model with X\n",
    "* fit_transform(X) : fits AND returns the transformed data\n",
    "* transform(X) : returns the transformed data using the fitted model\n",
    "* inverse_transform(X) : transform your data back to the original space\n",
    "* get_covariance() : computes the covariance matrix $cov \\in \\mathscr{M}_{n_{features}}$  \n",
    "$$cov =  components^T * S^2 * components + \\boldsymbol{\\sigma_2} * I_{n_{features}}$$ \n",
    "where $S^2$ contains the explained variances, and $\\boldsymbol{\\sigma_2}$ contains the noise variances.\n",
    "* get_precision() : computes the precision (inverse of the covariance)\n",
    "\n",
    "If you're inteerested in only a certain part of the whole dataset you can use the \n",
    "* svd_solver='randomized' : it only uses the right amount of data to predict the n_features wanted\n",
    "\n",
    "#### Incremental PCA\n",
    "\n",
    "For big sized data you would want to use chunks of data.\n",
    "It computes estimates of components and noise variances from a batch and then updates them with the next batch <br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html\n",
    "\n",
    "#### Kernel PCA\n",
    "\n",
    "You can use a special kernel to separate non linear datasets :\n",
    "* kernel : “linear” | “poly” | “rbf” | “sigmoid” | “cosine” | “precomputed” : those are all the different kernel available <br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA\n",
    "\n",
    "\n",
    "#### Sparse PCA\n",
    "\n",
    "You can use Sparse PCA to yield sparse component, this is used via a Lasso ($l_1$) regularization\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparsePCA.html#sklearn.decomposition.SparsePCA\n",
    "\n",
    "#### Truncated SVD\n",
    "\n",
    "If you have a large sparse dataset that you don't want to center (because of Out Of Memory Error) use this algorithm (ex : tf-idf count matrices)\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
