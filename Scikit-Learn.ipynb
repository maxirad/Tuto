{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Scikit-Learn\n",
    "\n",
    "\n",
    "# Dimension Reduction\n",
    "\n",
    "https://scikit-learn.org/stable/modules/decomposition.html <br>\n",
    "\n",
    "### [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "\n",
    "The point is to find the successive orthogonal components that explain most of the variance of the centered data set.\n",
    "Here is a very simple video on the Topic https://www.youtube.com/watch?v=FgakZw6K1QQ\n",
    "\n",
    "you can specify in n_components\n",
    "* number of features to keep\n",
    "* 'mle' to let Minka's MLE algorithm fit it for you https://vismod.media.mit.edu/tech-reports/TR-514.pdf\n",
    "* a percentage between 0 and 1 that represents the amount of total variance that should be explained by your features\n",
    "\n",
    "Useful attributes\n",
    "* components_ : array, shape (n_components, n_features) -- Gives you the n_components components (rows) and the contribution of each feature (columns)\n",
    "* explained_variance_ (ratio_) : array, shape (n_components,) -- Gives you the variance explained by each component\n",
    "\n",
    "Some Methods\n",
    "* fit(X) : fits the model with X\n",
    "* fit_transform(X) : fits AND returns the transformed data\n",
    "* transform(X) : returns the transformed data using the fitted model\n",
    "* inverse_transform(X) : transform your data back to the original space\n",
    "* get_covariance() : computes the covariance matrix $cov \\in \\mathscr{M}_{n_{features}}$  \n",
    "$$cov =  components^T * S^2 * components + \\boldsymbol{\\sigma_2} * I_{n_{features}}$$ \n",
    "where $S^2$ contains the explained variances, and $\\boldsymbol{\\sigma_2}$ contains the noise variances.\n",
    "* get_precision() : computes the precision (inverse of the covariance)\n",
    "\n",
    "If you're inteerested in only a certain part of the whole dataset you can use the \n",
    "* svd_solver='randomized' : it only uses the right amount of data to predict the n_features wanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_components' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-639c946718d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## X is the dataset : lines are instances, columns are features ##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mX_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_components' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "## X is the dataset : lines are instances, columns are features ##\n",
    "\n",
    "pca = PCA(n_components).fit(X)\n",
    "X_pca = PCA(n_components).transform(X)\n",
    "\n",
    "X_pca = PCA(n_components).fit_transform(X)\n",
    "\n",
    "# This function plots an elbow curve representing the variance explained by components\n",
    "def plot_elbow(X,n_components=10):\n",
    "    pca = PCA(n_components).fit(X)\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('number of components')\n",
    "    plt.ylabel('cumulative explained variance')\n",
    "    plt.title('Ratio of variance explained by the number components')\n",
    "    plt.show()\n",
    "    \n",
    "#A more general implementation for visualizing data is available under Kernel PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incremental PCA\n",
    "\n",
    "For big sized data you would want to use chunks of data.\n",
    "It computes estimates of components and naoise variances from a batch and then updates them with the next batch <br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel PCA\n",
    "\n",
    "Documentation : https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html\n",
    "\n",
    "- You can use a special kernel to separate non linear datasets : https://scikit-learn.org/stable/modules/metrics.html\n",
    "\n",
    "    - Linear : $$ K(x,x') = x^Tx' $$\n",
    "    - poly : $$ K(x,x') = ( \\color {green} \\gamma x^T x' + \\color {blue} c_0)^\\color {red}d $$\n",
    "    - sigmoid : $$ K(x,x') = tanh( \\color {green} \\gamma x^T x' + \\color {blue} c_0 ) \\;\\;\\; $$\n",
    "    - Radial basis function (RBF) : $$ K(x,x') = exp(- \\color {green} \\gamma \\|{x-x'}\\|^2) $$\n",
    "    - cosine : $$ K(x,x') = \\frac {x^T x'}{\\|x^T\\| \\|x'\\|} $$\n",
    "\n",
    "You can tune some Hyper parameter\n",
    "\n",
    "$\\color {green} \\gamma $ <br>\n",
    "`gamma  (default = 1/n_features) is used by poly / sigmoid / rbf`<br>\n",
    "$\\color {blue} {c_0} $ <br>\n",
    "`coef0  (default = 1)            is used by poly / sigmoid` <br>\n",
    "$\\color {red} d $ <br>\n",
    "`degree (default = 3)            is used by poly`<br>\n",
    "\n",
    "\n",
    "More info on kernels : http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# This function plots the projection of the data on the 1 2 or 3 main components and returns the PCA\n",
    "#using whichever kernel and parameter you give it\n",
    "\n",
    "def plot_pca (X,y,kernel='linear',n_components=2,gamma=None,coef0=None,degree=None):\n",
    "    pca = KernelPCA(n_components,kernel, gamma=gamma, degree=degree, coef0=coef0)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    print(\"original shape:   \", X.shape)\n",
    "    print(\"transformed shape:\", X_pca.shape)\n",
    "    if n_components==1:\n",
    "        plt.scatter(X_pca[:,0],np.zeros(len(X_pca),),alpha=0.2,c=y.values,vmin=-3,vmax=3,)\n",
    "        plt.xlabel('Component 1')\n",
    "        plt.title(\"data projected on the main component \\n using \" + kernel + \" kernel\")\n",
    "    elif n_components==2:\n",
    "        plt.scatter(X_pca[:,0],X_pca[:,1],alpha=0.2,c=y.values,vmin=-3,vmax=3)\n",
    "        plt.xlabel('Component 1')\n",
    "        plt.ylabel('Component 2')\n",
    "        plt.title(\"data projected on the 2 main components \\n using \" + kernel + \" kernel\")\n",
    "    elif n_components==3:\n",
    "        from mpl_toolkits.mplot3d import Axes3D\n",
    "        fig=plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(X_pca[:,0],X_pca[:,1],X_pca[:,2],alpha=0.2,c=y.values,vmin=-3,vmax=3)\n",
    "        ax.set_xlabel('Component 1')\n",
    "        ax.set_ylabel('Component 2')\n",
    "        ax.set_zlabel('Component 3')\n",
    "        plt.title(\"data projected on the 3 main components \\n using \" + kernel + \" kernel\")\n",
    "        return pca\n",
    "    else :\n",
    "        print(\"how am I supposed to show you that with your 2-D eyes, beta !\")\n",
    "        return pca\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.show()\n",
    "    return pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse PCA\n",
    "\n",
    "You can use Sparse PCA to yield sparse component, this is used via a Lasso ($l_1$) regularization\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparsePCA.html#sklearn.decomposition.SparsePCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truncated SVD\n",
    "\n",
    "If you have a large sparse dataset that you don't want to center (because of Out Of Memory Error) use this algorithm (ex : tf-idf count matrices)\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally Linear Embedding (LLE)\n",
    "\n",
    "Manifold Learning technique : Learning locally linear space for k closest neighbor $LLE : x^{(i)} \\to z^{(i)}$\n",
    "- Selects k closest neighbors \n",
    "$$ K_\\boldsymbol{x^{(i)}} = \\underset{(\\boldsymbol x^{(j)})_{j\\in[[1:k]]}}{\\operatorname{argmin}}\n",
    "(\\sum\\limits_{j=1}^k d(x^{(i)}-x^{(j)})) $$\n",
    "- Optimizes the weights for the locally linear relations (constructing linear model for each k subset)\n",
    "$$ \\boldsymbol {\\hat W} = \\underset{\\boldsymbol W \\in \\mathscr M_m}{\\operatorname{argmin}}\n",
    "\\sum\\limits_{i=1}^m \\| \\boldsymbol {x^{(i)}} - \\sum\\limits_{j=1}^m w_{i,j} \\boldsymbol{x^{(j)}} \\|^2  $$ \n",
    "$$ \\text{ where } w_{i,j}=0 \\text{ if } \\boldsymbol{x^{(j)}} \\not\\in K_\\boldsymbol{x^{(i)}} \n",
    "\\text{ and } \\sum\\limits_{j=1}^m w_{i,j}=1 $$\n",
    "- Minimizes the distance between the closest neighbourg (constructing low dimensional representation)\n",
    "$$ \\boldsymbol {\\hat Z} = \\underset{\\boldsymbol Z \\in \\mathscr M_m}{\\operatorname{argmin}}\n",
    "\\sum\\limits_{i=1}^m \\| \\boldsymbol {z^{(i)}} - \\sum\\limits_{j=1}^m \\hat{w}_{i,j} \\boldsymbol{z^{(j)}} \\|^2  $$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiDimensional Scaling (MDS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isomap\n",
    "\n",
    "Creates a graph and reduces dimensionality by preserving geodesic distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis (LDA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "Documentation :https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html\n",
    "\n",
    "## Grid Search\n",
    "\n",
    "Documentation : https://scikit-learn.org/stable/modules/grid_search.html\n",
    "\n",
    "A Grid Search is used to fine the best hyperparameter for your model.\n",
    "\n",
    "* a parameter space (which parameters of your are you gonna tune)\n",
    "* a method for searching and sampling candidates (which values are the parameters gonna take)\n",
    "* an estimator (what regressor or classifier will make the predictions)\n",
    "* a score function (how are you gonna measure which model is better)\n",
    "* a cross validation scheme (for unbiased estimator you have to cross validate)\n",
    "\n",
    "### Defining a grid of parameters\n",
    "\n",
    "Here is a standard parameter grid for a kernel PCA decomposition problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'pca__kernel': ['linear','cosine'],\n",
    "     'pca__n_components':[1,2,3,4,5,6]},\n",
    "    {'pca__kernel': ['rbf'], \n",
    "     'pca__gamma':[10**-6, 10**-5, 10**-4, 0.001, 0.01, 0.1, 1, 10],\n",
    "     'pca__n_components':[1,2,3,4,5,6]},\n",
    "    {'pca__kernel': ['sigmoid'],\n",
    "     'pca__gamma':[-10**-6,-10**-5,-10**-4,-0.001,-0.01,-0.1,-1,-10,\n",
    "                    10**-6, 10**-5, 10**-4, 0.001, 0.01, 0.1, 1, 10],\n",
    "     'pca__coef0':[-100,-10,-5,-1,-0.1,0,\n",
    "                    100, 10, 5, 1, 0.1],\n",
    "     'pca__n_components':[1,2,3,4,5,6]},\n",
    "    {'pca__kernel': ['poly'],\n",
    "     'pca__gamma':[10**-6, 10**-5, 10**-4, 0.001, 0.01, 0.1, 1, 10],\n",
    "     'pca__coef0':[-100,-10,-5,-1,-0.1,0,\n",
    "                    100, 10, 5, 1, 0.1],\n",
    "     'pca__degree':[-5,-4,-3,-2,-1,-0.5,0.5,2,3,4,5],\n",
    "     'pca__n_components':[1,2,3,4,5,6]},\n",
    " ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the Grid Search\n",
    "\n",
    "[GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)\n",
    "lets you choose several of the options you want for your Grid Search\n",
    "\n",
    "* estimator (object with a score function)\n",
    "* param_grid (dict)\n",
    "* n_jobs (int) : number of jobs to run in parallel : -1 sets maximum\n",
    "* cv (int) : number of fold for the Kfold or Stratified Kfold (default) or cv method\n",
    "* verbose (int) : 0 (no output) 1(some outout) 2(every CV time output) 3(CV time + score output)\n",
    "\n",
    "Useful attribute :\n",
    "* cv_results : Dict with results\n",
    "* best_estimator_ : object estimator with the parameters that yielded the best score\n",
    "\n",
    "Methods :\n",
    "* fit(X,y) : Runs fits for all the parameters\n",
    "* transform(X) : Runs transform of X for the best estimator\n",
    "* predict(X) : Runs predict of X using the best estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "Documentation https://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9,10], [11,12]])  #your EARLY dataset\n",
    "y = np.array([0, 1, 2, 3, 4, 5])                              #your PREDICTED dataset\n",
    "kf = KFold(n_splits=3)   #do a 3 fold\n",
    "print(X.shape, y.shape)\n",
    "scores=list()\n",
    "\n",
    "for train_index, test_index in kf.split(X,y):\n",
    "    print(\"TRAINindex:\", train_index, \"TESTindex:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(\"TrainSet: \\n\", X_train, \"\\n\", y_train,\"\\n TestSet: \\n\",X_test, \"\\n\",y_test)\n",
    "    \n",
    "    # DEFINE A MODEL HERE\n",
    "    \n",
    "    # FIT A MODEL HERE ON X_TRAIN + y_train\n",
    "    \n",
    "    # EVALUATE MODEL HERE X_TEST + y_test\n",
    "    \n",
    "    # STORE THE RESULTS in a list scores=list() scores.append(accuracy,loss)\n",
    "    \n",
    "print('Estimated Accuracy %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
